{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Loading libraries\n",
    "from urllib2 import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd, numpy as np\n",
    "import datetime, time, os, io, codecs, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the NYT site and query its API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Register for an API Key from the NYT developer website and enter your api key here\n",
    "apiKey = \"<your api key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Helper function to get json into a form I can work with  \n",
    "def convert(input):\n",
    "    if isinstance(input, dict):\n",
    "        return {convert(key): convert(value) for key, value in input.iteritems()}\n",
    "    elif isinstance(input, list):\n",
    "        return [convert(element) for element in input]\n",
    "    elif isinstance(input, unicode):\n",
    "        return input.encode('utf-8')\n",
    "    else:\n",
    "        return input\n",
    "\n",
    "### Helpful function to figure out what to name individual JSON files        \n",
    "def getJsonFileName(YR, MO, json_file_path):\n",
    "    json_file_name = \".\".join([YR+'-'+MO,'json'])\n",
    "    json_file_name = \"\".join([json_file_path,json_file_name])\n",
    "    return json_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to grab and save data\n",
    "def _grab_nyt_by_month_(YR, MO, apiKey):\n",
    "    ### Create a request string with the month and apiKey\n",
    "    request_string = \"https://api.nytimes.com/svc/archive/v1/\"+YR+\"/\"+MO+\".json?api-key=\"+apiKey\n",
    "    \n",
    "    ### Read the NYT site with the request string\n",
    "    response = urlopen(request_string)\n",
    "    content = response.read()\n",
    "    \n",
    "    ### Convert scraping results into a json format\n",
    "    articles = convert(json.loads(content))\n",
    "\n",
    "    ### Save results to a json file\n",
    "    if len(articles[\"response\"][\"docs\"]) >= 1:\n",
    "        json_file_name = getJsonFileName(YR, MO, 'D:\\Data\\NYTbyMonth\\NYT-')\n",
    "        json_file = open(json_file_name, 'w')\n",
    "        json_file.write(content)\n",
    "        json_file.close()\n",
    "    ### if no more articles, go to next month\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to read saved json files and convert them to dataframe\n",
    "def readJsonFileToDF(YR, MO):\n",
    "    ### Read saved json files and convert them into json format\n",
    "    file_name = getJsonFileName(YR, MO, 'D:\\Data\\NYTbyMonth\\NYT-')\n",
    "    in_file = open(file_name, 'r')\n",
    "    articles = convert(json.loads(in_file.read()))\n",
    "    in_file.close()\n",
    "    ### Create a dataframe from json files\n",
    "    artlist = articles[\"response\"][\"docs\"]\n",
    "    df = pd.DataFrame(artlist)\n",
    "    df.to_csv('D:/Data/NYTbyMonthDF/NYT_art_list_'+YR+'-'+MO+'.txt',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read article list and get URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Data/NYTimesArticleList.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "dfNew = df[['headline','pub_date','web_url']].copy()\n",
    "dfNew = dfNew.sort_values(by=['pub_date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Convert publication date column to string\n",
    "for index,row in dfNew.iterrows():\n",
    "    d = pd.to_datetime(row['pub_date'])\n",
    "    row['pub_date'] = d.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that grabs full articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _grab_full_nyt_(url):\n",
    "    \n",
    "    try:\n",
    "        sc = urlopen(url).read()\n",
    "        sc = bs(sc,'lxml')\n",
    "    except:\n",
    "        return url\n",
    "    \n",
    "    try:\n",
    "        titleline = sc.find_all('h1')\n",
    "        title = titleline[0].text.strip()\n",
    "    except:\n",
    "        title = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        authorline = sc.find_all('p',{'class':'byline-dateline'})\n",
    "        if len(authorline)>0:\n",
    "            result = []\n",
    "            for a in authorline:\n",
    "                result.extend(a.find_all('span',{'class':'byline'}))\n",
    "            author = [a.text.strip() for a in result]\n",
    "            author = u' '.join(author)\n",
    "        else:\n",
    "            authorline = sc.find_all('p',{'class':'css-1cbhw1y e1x1pwtg1'})\n",
    "            author = authorline[0].text.strip()\n",
    "    except:\n",
    "        author = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        bodyline = sc.find_all('p',{'class':'story-body-text'})\n",
    "        if len(bodyline)==0:\n",
    "            bodyline = sc.find_all('p',{'class':'css-1tyen8a e2kc3sl0'})\n",
    "        content = [a.text.strip() for a in bodyline]\n",
    "        content = u' '.join(content)\n",
    "    except:\n",
    "        content = 'N/A'\n",
    "    \n",
    "    return [title,author,content,sc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [d.strftime('%Y-%m-%d') for d in pd.date_range('2009-01-01','2009-01-03',freq='D')]\n",
    "dates = sorted(set(dates))     # Eliminate duplicates from dates and sort the new set\n",
    "dates[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [(dt, list(dfNew.loc[dfNew.pub_date == dt,'web_url'])) for dt in dates]     # Get list of dates and urllists by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datelist, urllist = zip(*inputs)     # Unzip packet to dates and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secs = 2\n",
    "urlerrors = list()\n",
    "for i,d in enumerate(datelist):\n",
    "    urls = urllist[i]\n",
    "    if len(urls)==0:\n",
    "        continue\n",
    "    data = list()\n",
    "    for url in urls:\n",
    "        stuff = _grab_full_nyt_(url)\n",
    "        if type(stuff)==list:\n",
    "            data.append(stuff+[url])\n",
    "        else:\n",
    "            urlerrors.append([stuff,d])\n",
    "#         data.append(_grab_full_nyt_(url)+[url])\n",
    "        time.sleep(secs)\n",
    "    df = pd.DataFrame(data)\n",
    "    df['date'] = d\n",
    "    df.columns = ['title','author','content','sourcecode','url','date']\n",
    "    df.to_csv('D:/Data/NYTFullwithSourceCode/nyt_full_'+d+'.txt',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnames = os.listdir('D:/Data/NYTFullwithSourceCode/')\n",
    "df = list()\n",
    "for f in fnames:\n",
    "    df.append(pd.read_csv('D:/Data/NYTFullwithSourceCode/'+f,encoding='utf-8',sep='|',index_col=0))\n",
    "df = pd.concat(df)\n",
    "df1 = df.sort_values(by=['date'])\n",
    "df1.index = range(len(df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parallel code for grabbing full articles on starcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This part runs on Amazon Web Services (AWS) by a starcluster\n",
    "import os, pandas as pd , time, datetime\n",
    "import ipyparallel as ipp\n",
    "from ipyparallel import Client\n",
    "# client = Client() # run on local ipcluster\n",
    "client = Client('Your security group',\n",
    "                sshkey='Your SSH key')\n",
    "lbview = client.load_balanced_view()\n",
    "pnodes = len(client.ids)     # Number of nodes in the starcluster\n",
    "print pnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function that write a file to all nodes in starcluster\n",
    "@lbview.parallel(block=True)\n",
    "def write_to_starcluster(s,filepath='/home/sgeadmin/temp.py'):\n",
    "    import time\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        with open(filepath,'r') as f:         # Open filepath to read files\n",
    "            scopy = f.read()\n",
    "        if scopy == s:                        # If file s already exists\n",
    "            return 'already on server'\n",
    "        else:\n",
    "            with open(filepath,'w') as f:     # Open filepath to write files\n",
    "                f.write(s)                    # Write file s to server\n",
    "            return 'written to server'\n",
    "    except:\n",
    "        with open(filepath,'w') as f:\n",
    "                f.write(s)\n",
    "        return 'written to server'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function that reads files from all nodes in starcluster\n",
    "@lbview.parallel(block = True)\n",
    "def read_starcluster(s):\n",
    "    import os\n",
    "    files = os.listdir('/home/sgeadmin')\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('D:/Codes/_s3_IO_.py') as f:     # Open file _s3_IO_.py from local computer to read\n",
    "    s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_starcluster.map([s]*pnodes*3)     # Write file _s3_IO_.py to all nodes in starcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@lbview.parallel(block=True)\n",
    "def _execute_nyt_(packet):\n",
    "    from urllib2 import urlopen                   # Open arbitrary resources by URL\n",
    "    from bs4 import BeautifulSoup as bs           # Library for pulling data out of HTML and XML files\n",
    "    import pandas as pd                           # Data manipulation and analysis\n",
    "    import time                                   # Time access and conversions\n",
    "    import datetime                               # Manipulating dates and times\n",
    "    import os                                     # Miscellaneous operating system interfaces\n",
    "    \n",
    "    with open('/home/sgeadmin/temp.py') as f:     # Open files in starcluster to read\n",
    "        s = f.read()\n",
    "    exec(s, globals())                            # Execute files\n",
    "    \n",
    "    dates, urllists = zip(*packet)                # Unzip packet to dates and urls\n",
    "    \n",
    "    ### Function that grabs full articles\n",
    "    def _grab_full_nyt_(url):\n",
    "        \n",
    "        try:\n",
    "            sc = urlopen(url).read()\n",
    "            sc = bs(sc,'lxml')\n",
    "        except:\n",
    "            return url\n",
    "        \n",
    "        try:\n",
    "            titleline = sc.find_all('h1')\n",
    "            title = titleline[0].text.strip()\n",
    "        except:\n",
    "            title = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            authorline = sc.find_all('p',{'class':'byline-dateline'})\n",
    "            if len(authorline)>0:\n",
    "                result = []\n",
    "                for a in authorline:\n",
    "                    result.extend(a.find_all('span',{'class':'byline'}))\n",
    "                author = [a.text.strip() for a in result]\n",
    "                author = u' '.join(author)\n",
    "            else:\n",
    "                authorline = sc.find_all('p',{'class':'css-1cbhw1y e1x1pwtg1'})\n",
    "                author = authorline[0].text.strip()\n",
    "        except:\n",
    "            author = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            bodyline = sc.find_all('p',{'class':'story-body-text'})\n",
    "            if len(bodyline)==0:\n",
    "                bodyline = sc.find_all('p',{'class':'css-1tyen8a e2kc3sl0'})\n",
    "            content = [a.text.strip() for a in bodyline]\n",
    "            content = u' '.join(content)\n",
    "        except:\n",
    "            content = 'N/A'\n",
    "        \n",
    "        return [title,author,content,sc]\n",
    "    \n",
    "    secs = 2\n",
    "    outputs = list()\n",
    "    urlerrors = list()\n",
    "    for i,d in enumerate(dates):\n",
    "        urls = urllists[i]\n",
    "        if len(urls)==0:\n",
    "            continue\n",
    "        data = list()\n",
    "        for url in urls:\n",
    "            stuff = _grab_full_nyt_(url)\n",
    "            if type(stuff)==list:\n",
    "                data.append(stuff+[url])\n",
    "            else:\n",
    "                urlerrors.append([stuff,d])\n",
    "#             data.append(_grab_full_nyt_(url)+[url])\n",
    "            time.sleep(secs)\n",
    "        df = pd.DataFrame(data)\n",
    "        df['date'] = d\n",
    "        df.columns = ['title','author','content','sourcecode','url','date']\n",
    "        ### Save to s3\n",
    "        dfstring = df.to_csv(encoding='utf-8',sep='|')\n",
    "        try:\n",
    "            _write_file_from_str_to_s3_(dfstring,'Data/NYTFull/nyt_full_'+d+'.txt',public=False,html=False)\n",
    "            out = 'Data/NYTFull/nyt_full_'+d+'.txt'\n",
    "        except:\n",
    "            out = 'failed to write {}'.format(d)\n",
    "        outputs.append(out)\n",
    "    return outputs, urlerrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i + n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [d.strftime('%Y-%m-%d') for d in pd.date_range('2009-06-01','2017-12-31',freq='D')]\n",
    "dates = sorted(set(dates))     # Eliminate duplicates from dates and sort the new set\n",
    "dates[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [(dt, list(dfNew.loc[dfNew.pub_date == dt,'web_url'])) for dt in dates]     # Get list of dates and urllists by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterspernode = 1\n",
    "ndates = [inp for inp in inputs if inp[0] in dates]                       # Get nonempty list of dates and urllists\n",
    "ndates = chunks(ndates,max(int(len(ndates)/(pnodes*iterspernode)),1))     # Split dates for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'each node gets {} dates'.format(len(ndates[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = _execute_nyt_.map(ndates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Filenames in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto\n",
    "import boto.s3.connection\n",
    "\n",
    "conn = boto.connect_s3()\n",
    "bucket = conn.get_bucket('Your bucket',validate=False)\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3filenames = []\n",
    "for key in bucket.list(prefix='Data/NYTFull'):\n",
    "    keyname = str(key)[37:-5]\n",
    "    s3filenames.append(keyname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnames = os.listdir('D:/Data/NYTFullwithSourceCode/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finished = [x[9:-4] for x in fnames]\n",
    "unfinished = [x for x in dates if x not in s3filenames and x not in finished]\n",
    "dates = unfinished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Documentation (accessing S3 Data in Python with Boto3): https://dluo.me/s3databoto3\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "config = Config(connect_timeout=120,read_timeout=120)     # Prevent timeout error when reading big files\n",
    "s3client = boto3.client('s3',config=config)               # Low-level functional-oriented API\n",
    "s3resource = boto3.resource('s3',config=config)           # High-level object-oriented API\n",
    "s3bucket = s3resource.Bucket('Your bucket')                  # Indicate bucket name\n",
    "s3bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.compat import StringIO\n",
    "import datetime, time, os, io, codecs\n",
    "\n",
    "\"\"\"\n",
    "Iterates through all the objects, doing the pagination for you. Each object is an s3.ObjectSummary, so it doesn't\n",
    "contain the body. You need to call get() to get the whole body\n",
    "\"\"\"\n",
    "s3keys = list()\n",
    "s3df = list()\n",
    "for obj in s3bucket.objects.filter(Prefix='Data/NYTFull/'):   # Use s3bucket.objects.all() for all objects\n",
    "    key = obj.key\n",
    "    s3keys.append(key)\n",
    "    body = obj.get()['Body'].read()\n",
    "    testdata = StringIO(body)\n",
    "    dat = pd.read_csv(testdata,encoding='utf-8',sep='|',index_col=0)\n",
    "#     with codecs.open('./s3temp.csv','w') as f:\n",
    "#         f.write(body)\n",
    "#     dat = pd.read_csv('./s3temp.csv',encoding='utf-8',sep='|',index_col=0)\n",
    "    s3df.append(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3df = pd.concat(s3df)\n",
    "s3dfNew = s3df.sort_values(by=['date'])\n",
    "s3dfNew.index = range(len(s3df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read full-text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function that gets title, author, and content from sourcecode\n",
    "def _get_dataframe_nyt_(sourcecode):\n",
    "\n",
    "    try:\n",
    "        sc = sourcecode\n",
    "        sc = bs(sc,'lxml')\n",
    "    except:\n",
    "        sc = 'N/A'\n",
    "\n",
    "    try:\n",
    "        titleline = sc.find_all('h1')\n",
    "        title = titleline[0].text.strip()\n",
    "    except:\n",
    "        title = 'N/A'\n",
    "\n",
    "    try:\n",
    "        authorline = sc.find_all('p',{'class':'byline-dateline'})\n",
    "        if len(authorline)>0:\n",
    "            result = []\n",
    "            for a in authorline:\n",
    "                result.extend(a.find_all('span',{'class':'byline'}))\n",
    "            author = [a.text.strip() for a in result]\n",
    "            author = u' '.join(author)\n",
    "        else:\n",
    "            authorline = sc.find_all('p',{'class':'css-1cbhw1y e1x1pwtg1'})\n",
    "            author = authorline[0].text.strip()\n",
    "    except:\n",
    "        author = 'N/A'\n",
    "\n",
    "    try:\n",
    "        bodyline1 = sc.find_all('p',{'class':'story-body-text'})\n",
    "        if len(bodyline1)!=0:\n",
    "            content = [a.text.strip() for a in bodyline1]\n",
    "        else:\n",
    "            bodyline2 = sc.find_all('p',{'class':'css-1tyen8a e2kc3sl0'})\n",
    "            if len(bodyline2)!=0:\n",
    "                content = [a.text.strip() for a in bodyline2]\n",
    "            else:\n",
    "                bodyline3 = sc.find_all('p',{'class':'css-1i0edl6 e2kc3sl0'})\n",
    "                if len(bodyline3)!=0:\n",
    "                    content = [a.text.strip() for a in bodyline3]\n",
    "                else:\n",
    "                    bodyline4 = sc.find_all('p',{'class':'g-body'})\n",
    "                    if len(bodyline4)!=0:\n",
    "                        content = [a.text.strip() for a in bodyline4]\n",
    "#                     else:\n",
    "#                         content = 'N/A'\n",
    "        content = u' '.join(content)\n",
    "    except:\n",
    "        content = 'N/A'\n",
    "\n",
    "    return [title,author,content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnames = os.listdir('D:/Data/NYTFullwithSourceCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Read files with sourcecode, fill in missing content from sourcecode, and save new files without sourcecode\n",
    "for f in fnames:\n",
    "    df = pd.read_csv('D:/Dropbox/Data/NYTFullwithSourceCode/'+f,encoding='utf-8',sep='|',index_col=0)\n",
    "    data = list()\n",
    "    for index,row in df.iterrows():\n",
    "        sourcecode = row['sourcecode']\n",
    "        url = row['url']\n",
    "        d = row['date']\n",
    "        stuff = _get_dataframe_nyt_(sourcecode)\n",
    "        data.append(stuff+[url]+[d])\n",
    "    dfNew = pd.DataFrame(data)\n",
    "    dfNew.columns = ['title','author','content','url','date']\n",
    "    dfNew.to_csv('./NYTFullwithoutSourceCode/'+f,sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnames = os.listdir('./NYTFullwithoutSourceCode')\n",
    "df = list()\n",
    "for f in fnames:\n",
    "    df.append(pd.read_csv('./NYTFullwithoutSourceCode/'+f,encoding='utf-8',sep='|',index_col=0))\n",
    "df = pd.concat(df)\n",
    "df1 = df.sort_values(by=['date'])\n",
    "df1.index = range(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = [d.strftime('%Y-%m-%d') for d in pd.date_range('2009-01-01','2017-12-18',freq='D')]\n",
    "dates = sorted(set(dates))     # Eliminate duplicates from dates and sort the new set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get list of dates and urls of rows with empty content by date\n",
    "worklist = [(dt, list(dat.loc[dat.date==dt,'url'])) for dt in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datelists, urllists = zip(*worklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fill in more missing content from sourcecode and save new files without sourcecode\n",
    "for i,d in enumerate(datelists):\n",
    "    urls = urllists[i]\n",
    "    if len(urls)==0:\n",
    "        continue\n",
    "    else:\n",
    "        orgdf = pd.read_csv('D:/Dropbox/Data/NYTFullwithSourceCode/nyt_full_'+d+'.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "        repdf = pd.DataFrame()\n",
    "        for address in urls:\n",
    "            repdf = repdf.append(orgdf[orgdf['url']==address])\n",
    "        data = list()\n",
    "        for index,row in repdf.iterrows():\n",
    "            sourcecode = row['sourcecode']\n",
    "            url = row['url']\n",
    "            stuff = _get_dataframe_nyt_(sourcecode)\n",
    "            data.append(stuff+[url]+[d])\n",
    "        dfNew = pd.DataFrame(data)\n",
    "        dfNew.columns = ['title','author','content','url','date']\n",
    "        dfNew.to_csv('./NYTFullwithoutSourceCodeV1/nyt_full_'+d+'.txt',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnames = os.listdir('./NYTFullwithoutSourceCodeV1')\n",
    "dfV1 = list()\n",
    "for f in fnames:\n",
    "    dfV1.append(pd.read_csv('./NYTFullwithoutSourceCodeV1/'+f,encoding='utf-8',sep='|',index_col=0))\n",
    "dfV1 = pd.concat(dfV1)\n",
    "dfV1New = dfV1.sort_values(by=['date'])\n",
    "dfV1New.index = range(len(dfV1New))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace empty content with new content and save new dataframe\n",
    "fnames = os.listdir('./NYTFullwithoutSourceCodeV1')\n",
    "for f in fnames:\n",
    "    dfV1 = pd.read_csv('./NYTFullwithoutSourceCodeV1/'+f,encoding='utf-8',sep='|',index_col=0)\n",
    "    if dfV1['content'].count()==0:                                        # Count number of rows that are not NaN\n",
    "        continue\n",
    "    else:\n",
    "        dfV1.dropna(subset=['content'],inplace=True)                      # Drop rows that are NaN\n",
    "        dfV0 = pd.read_csv('./NYTFullwithoutSourceCode/'+f,encoding='utf-8',sep='|',index_col=0).append(dfV1)\n",
    "        dfV0.drop_duplicates(subset=['url'],keep='last',inplace=True)     # Remove rows with duplicate urls\n",
    "        dfV0.reset_index(drop=True,inplace=True)                          # Reset index of new dataframe\n",
    "        dfV0.to_csv('./NYTFullwithoutSourceCodeV2/'+f,sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnames = os.listdir('D:/Dropbox/Data/NYTFullwithoutSourceCode')\n",
    "df = list()\n",
    "for f in fnames:\n",
    "    df.append(pd.read_csv('D:/Dropbox/Data/NYTFullwithoutSourceCode/'+f,encoding='utf-8',sep='|',index_col=0))\n",
    "df = pd.concat(df)\n",
    "df1 = df.sort_values(by=['date'])\n",
    "df1.index = range(len(df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match full-text dataframe with original dataframe that has metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfOrg = pd.read_csv('D:/Data/NYTimesArticleList.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "dfFull = pd.read_csv('D:/Data/NYTFull.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "dfFull.dropna(subset=['content'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove rows with duplicate urls\n",
    "dfOrg1 = dfOrg.drop_duplicates(subset='web_url',keep='first')\n",
    "dfFull1 = dfFull.drop_duplicates(subset='url',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select rows from dfOrg1 that has the same urls as dfFull1\n",
    "df1 = dfOrg1[dfOrg1['web_url'].isin(dfFull1['url'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = list()\n",
    "for index,row in dfFull.iterrows():\n",
    "    title = row['title']\n",
    "    author = row['author']\n",
    "    content = row['content']\n",
    "    url = row['url']\n",
    "    date = row['date']\n",
    "    \n",
    "    matchrow = dfOrg[dfOrg['web_url'].str.contains(url)]\n",
    "    document_type = matchrow['document_type'].values[0]\n",
    "    type_of_material = matchrow['type_of_material'].values[0]\n",
    "    news_desk = matchrow['news_desk'].values[0]\n",
    "    headline = matchrow['headline'].values[0]\n",
    "    keywords = matchrow['keywords'].values[0]\n",
    "    print_page = matchrow['print_page'].values[0]\n",
    "    pub_date = matchrow['pub_date'].values[0]\n",
    "    section_name = matchrow['section_name'].values[0]\n",
    "    subsection_name = matchrow['subsection_name'].values[0]\n",
    "    source = matchrow['source'].values[0]\n",
    "    web_url = matchrow['web_url'].values[0]\n",
    "    word_count = matchrow['word_count'].values[0]\n",
    "    \n",
    "    matchlist = [title,author,content,url,date,document_type,type_of_material,news_desk,headline,keywords,\n",
    "                 print_page,pub_date,section_name,subsection_name,source,web_url,word_count]\n",
    "    data.append(matchlist)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['title','author','content','url','date','document_type','type_of_material','news_desk',\n",
    "              'headline','keywords','print_page','pub_date','section_name','subsection_name','source',\n",
    "              'web_url','word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort two dataframes by url\n",
    "df2 = df1.sort_values(by=['web_url']).reset_index(drop=True)\n",
    "dfFullNew = dfFull1.sort_values(by=['url']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select relevant columns from df2\n",
    "dfOrgNew = df2[['document_type','type_of_material','news_desk','headline','keywords','print_page',\n",
    "                'pub_date','section_name','subsection_name','source','web_url','word_count']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine two dataframes and sort the new dataframe by date\n",
    "df = pd.concat([dfFullNew,dfOrgNew],axis=1)\n",
    "df = df.sort_values(by=['date']).reset_index(drop=True)\n",
    "df1 = df[['title','author','content','date','document_type','type_of_material','news_desk',\n",
    "         'section_name','subsection_name','word_count']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4 = df1[['date','document_type']].copy()\n",
    "df4['date'] = pd.to_datetime(df4['date'])\n",
    "df4group = df4.groupby(pd.Grouper(key='date',freq='M'))\n",
    "df4grouplist = list(df4group['document_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finallist = list()\n",
    "for i in range(len(df4grouplist)):\n",
    "    date = df4grouplist[i][0]\n",
    "    doclist = list(df4grouplist[i][1])\n",
    "    articlecount = doclist.count('article')\n",
    "    blogpostcount = doclist.count('blogpost')\n",
    "    multimediacount = doclist.count('multimedia')\n",
    "    total = len(doclist)\n",
    "    artpercent = float(articlecount)/total\n",
    "    blogpercent = float(blogpostcount)/total\n",
    "    mediapercent = float(multimediacount)/total\n",
    "    finallist.append([date,articlecount,blogpostcount,multimediacount,total,artpercent,blogpercent,mediapercent])\n",
    "df4doc = pd.DataFrame(finallist)\n",
    "df4doc.columns = ['date','article_count','blogpost_count','multimedia_count','total_count',\n",
    "                 'article_percent','blogpost_percent','multimedia_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpercent = df4doc[['date','article_percent','blogpost_percent']].copy()\n",
    "dfpercent.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfpercent1 = dfpercent.groupby(pd.Grouper(key='date',freq='M')).mean()\n",
    "plt.plot(dfpercent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect English texts and remove non-English texts from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNYT = pd.read_csv('/mnt/data/NYTFullNew.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataNYT = dfNYT.copy()\n",
    "dataNYT.dropna(subset=['content'],inplace=True)\n",
    "dataNYT.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycld2 as cld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_chars = [u'\\x00', u'\\x01', u'\\x02', u'\\x03', u'\\x04', u'\\x05', u'\\x06', u'\\x07', u'\\x08', u'\\x0b', u'\\x0e',\n",
    "             u'\\x0f', u'\\x10', u'\\x11', u'\\x12', u'\\x13', u'\\x14', u'\\x15', u'\\x16', u'\\x17', u'\\x18', u'\\x19',\n",
    "             u'\\x1a', u'\\x1b', u'\\x1c', u'\\x1d', u'\\x1e', u'\\x1f', u'\\x7f', u'\\x80', u'\\x81', u'\\x82', u'\\x83',\n",
    "             u'\\x84', u'\\x85', u'\\x86', u'\\x87', u'\\x88', u'\\x89', u'\\x8a', u'\\x8b', u'\\x8c', u'\\x8d', u'\\x8e',\n",
    "             u'\\x8f', u'\\x90', u'\\x91', u'\\x92', u'\\x93', u'\\x94', u'\\x95', u'\\x96', u'\\x97', u'\\x98', u'\\x99',\n",
    "             u'\\x9a', u'\\x9b', u'\\x9c', u'\\x9d', u'\\x9e', u'\\x9f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Detect English texts using pycld2 on dataNYT\n",
    "count1 = 0\n",
    "EngNYT = []\n",
    "start_time1 = time.time()\n",
    "for index,row in dataNYT.iterrows():\n",
    "    title = row['title']\n",
    "    author = row['author']\n",
    "    text = row['content']\n",
    "    url = row ['url']\n",
    "    date = row['date']\n",
    "    document_type = row['document_type']\n",
    "    type_of_material = row['type_of_material']\n",
    "    news_desk = row['news_desk']\n",
    "    headline = row['headline']\n",
    "    keywords = row['keywords']\n",
    "    print_page = row['print_page']\n",
    "    pub_date = row['pub_date']\n",
    "    section_name = row['section_name']\n",
    "    subsection_name = row['subsection_name']\n",
    "    source = row['source']\n",
    "    web_url = row['web_url']\n",
    "    word_count = row['word_count']\n",
    "    count1 += 1\n",
    "    newtext = text\n",
    "    for item in bad_chars:\n",
    "        newtext = newtext.replace(item,' ')\n",
    "    t = newtext.encode('utf-8')\n",
    "    reliable, index, top_3_choices = cld2.detect(t)\n",
    "    lang = top_3_choices[0][1]\n",
    "    if lang=='en':\n",
    "        EngNYT.append([date,pub_date,title,headline,author,newtext,document_type,type_of_material,news_desk,\n",
    "                       keywords,section_name,subsection_name,source,print_page,word_count,url,web_url])\n",
    "run_time1 = time.time()-start_time1\n",
    "print(\"This takes %s seconds to run\" %run_time1)\n",
    "print \"Length of EngNYT = \",len(EngNYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataEngNYT = pd.DataFrame(EngNYT,columns=['date','pub_date','title','headline','author','content','document_type',\n",
    "                                          'type_of_material','news_desk','keywords','section_name','subsection_name',\n",
    "                                          'source','print_page','word_count','url','web_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataEngNYT = dataEngNYT.rename(columns={'newtext':'content'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Clean texts before running analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, datetime, time, os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF = pd.read_csv('/mnt/data/TextAnalysis-All/NYTFullEng.txt',encoding='utf-8',sep='|',index_col=0,chunksize=10000)\n",
    "DF = pd.concat(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove duplicate urls\n",
    "df = DF.drop_duplicates(subset='url',keep='first').reset_index(drop=True)\n",
    "# Drop rows that have NaN content\n",
    "df = df.dropna(subset=['content']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove rows reporting lottery numbers (1648)\n",
    "df = df[df['title']!='Lottery Numbers'].reset_index(drop=True)\n",
    "# Remove rows reporting operating hours on New Year's Day (2)\n",
    "df = df[df['title']!=u'New Year\\u2019s Day'].reset_index(drop=True)\n",
    "# Remove rows that are corrections (4330)\n",
    "df = df[df['title']!=u'Corrections']\n",
    "df = df[df['title']!=u'Correction'].reset_index(drop=True)\n",
    "# Remove rows that are Word+Quiz (261)\n",
    "indices = list()\n",
    "for index,row in df[702900:].iterrows():\n",
    "    if type(row['title'])!=float:\n",
    "        if u'Word + Quiz' in row['title']:\n",
    "#             print index\n",
    "            indices.append(index)\n",
    "df.drop(df.index[indices],inplace=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _clean_nyt_texts_(df):\n",
    "    texts = list()\n",
    "    for index,row in df.iterrows():\n",
    "        text0 = row['content']\n",
    "        if 0<=text0.find(u'\\u2014')<=30:\n",
    "            if text0[0:3].isupper():\n",
    "                text1 = text0[text0.find(u'\\u2014')+1:]\n",
    "            else:\n",
    "                text1 = text0\n",
    "        else:\n",
    "            text1 = text0\n",
    "        text2 = re.sub(r'\\b[A-Za-z0-9-_@,.!/]+.com\\b','',text1)\n",
    "        text2 = re.sub(r'\\b[A-Za-z0-9-_@,.!/]+.co.za\\b','',text2)\n",
    "        text2 = re.sub(r'\\b[A-Za-z0-9-_@,.!/]+.gov\\b','',text2)\n",
    "        text2 = re.sub(r'\\b[A-Za-z0-9-_@,.!/]+.org\\b','',text2)\n",
    "        text2 = re.sub(r'\\b[A-Za-z0-9-_@,.!/]+.htm\\b','',text2)\n",
    "        text2 = re.sub(r'\\b[A-Za-z0-9-_@,.!/]+.io\\b','',text2)\n",
    "        text3 = ' '.join(text2.split())\n",
    "        texts.append(text3)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "data1 = df.copy()\n",
    "result = _clean_nyt_texts_(data1)\n",
    "data1['cleaned_text'] = result\n",
    "oldcols = data1.columns.tolist()\n",
    "newcols = oldcols[:6]+oldcols[-1:]+oldcols[6:-1]\n",
    "data2 = data1[newcols]\n",
    "data2 = data2.sort_values(by=['date']).reset_index(drop=True)\n",
    "print(\"This takes %s seconds to run\" %(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNYT = pd.read_csv('/mnt/data/TextAnalysis-All/NYTcleaned.txt',encoding='utf-8',sep='|',index_col=0,chunksize=10000)\n",
    "dfNYT = pd.concat(dfNYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dfNYT[['date','title','author','content','cleaned_text','document_type','type_of_material']].copy()\n",
    "df = dfNYT.copy()\n",
    "# Remove rows [300072,331117,452168] that have almost no content after being cleaned again\n",
    "df.drop(df.index[[300072,331117,452168]],inplace=True)\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_texts = list()\n",
    "for index,row in df.iterrows():\n",
    "    lower_text = row['cleaned_text'].lower()\n",
    "    lower_texts.append(lower_text)\n",
    "len(lower_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1['lower_text'] = lower_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newtexts = list(); indices1 = list(); indices2 = list(); indices3 = list(); indices4 = list(); indices5 = list()\n",
    "indices6 = list(); indices7 = list(); indices8 = list(); indices9 = list(); indices10 = list(); indices11 = list()\n",
    "indices12 = list(); indices13 = list(); indices14 = list(); indices15 = list()\n",
    "\n",
    "for index,row in df1.iterrows():\n",
    "    cleaned_text = row['cleaned_text']\n",
    "#     lower_text = row['lower_text']\n",
    "    text1 = cleaned_text\n",
    "    if cleaned_text.find(u'NYTD.fullScreenSlideShowFactory.createSlideshow')>=0:\n",
    "        while text1.find(u'NYTD.fullScreenSlideShowFactory.createSlideshow')>=0:\n",
    "            start1 = text1.find(u'NYTD.fullScreenSlideShowFactory.createSlideshow')\n",
    "            end1 = start1+text1[start1:].find(u';')+1\n",
    "            term1 = text1[start1:end1]\n",
    "            text1 = text1.replace(term1,'')\n",
    "        indices1.append(index)\n",
    "    if text1.find(u'var data;')>=0:\n",
    "        start2 = text1.find(u'var data;')\n",
    "        end2 = text1[:start2+300].rfind(u'}')+1\n",
    "        term2 = text1[start2:end2]\n",
    "        text2 = text1.replace(term2,'')\n",
    "        indices2.append(index)\n",
    "    else:\n",
    "        text2 = text1\n",
    "    if text2.find(u'$.noConflict()')>=0:\n",
    "        start3 = text2.find(u'$.noConflict()')\n",
    "        end3 = text2[:start3+780].rfind(u'});')+len(u'});')\n",
    "        term3 = text2[start3:end3]\n",
    "        text3 = text2.replace(term3,'')\n",
    "        indices3.append(index)\n",
    "    else:\n",
    "        text3 = text2\n",
    "    text4 = text3\n",
    "    if text3.find(u'jQuery(document)')>=0:\n",
    "        while text4.find(u'jQuery(document)')>=0:\n",
    "            start4 = text4.find(u'jQuery(document)')\n",
    "            end4 = start4+text4[start4:].find(u'});')+len(u'});')\n",
    "            term4 = text4[start4:end4]\n",
    "            text4 = text4.replace(term4,'')\n",
    "        indices4.append(index)\n",
    "    text5 = text4\n",
    "    if text4.find(u'(function($)')>=0:\n",
    "        while text5.find(u'(function($)')>=0:\n",
    "            start5 = text5.find(u'(function($)')\n",
    "            end5 = start5+text5[start5:].find(u'NYTD.jQuery);')+len(u'NYTD.jQuery);')\n",
    "            term5 = text5[start5:end5]\n",
    "            text5 = text5.replace(term5,'')\n",
    "        indices5.append(index)\n",
    "    if text5.find(u'NYTD.jQuery = jQuery.noConflict()')>=0:\n",
    "        start6 = text5.find(u'NYTD.jQuery = jQuery.noConflict()')\n",
    "        end6 = start6+text5[start6:].find(u'});')+len(u'});')\n",
    "        term6 = text5[start6:end6]\n",
    "        text6 = text5.replace(term6,'')\n",
    "        indices6.append(index)\n",
    "    else:\n",
    "        text6 = text5\n",
    "    if text6.find(u'NYTD.hotSpotsFactory')>=0:\n",
    "        start7 = text6.find(u'NYTD.hotSpotsFactory')\n",
    "        end7 = start7+text6[start7:].find(u';')+1\n",
    "        term7 = text6[start7:end7]\n",
    "        text7 = text6.replace(term7,'')\n",
    "        indices7.append(index)\n",
    "    else:\n",
    "        text7 = text6\n",
    "    if text7.find(u'new NYTD.NYTINT')>=0:\n",
    "        start8 = text7.find(u'new NYTD.NYTINT')\n",
    "        end8 = start8+text7[start8:].find(u'.photos);')+len(u'.photos);')\n",
    "        if end8<=start8+len(u'.photos);'):\n",
    "            end8 = start8+text7[start8:].find(u'fetch();')+len(u'fetch();')\n",
    "        term8 = text7[start8:end8]\n",
    "        text8 = text7.replace(term8,'')\n",
    "        indices8.append(index)\n",
    "    else:\n",
    "        text8 = text7\n",
    "    if text8.find(u'new NYTD.NYTMM')>=0:\n",
    "        start9 = text8.find(u'new NYTD.NYTMM')\n",
    "        end9 = start9+text8[start9:].find(u'});')+len(u'});')\n",
    "        term9 = text8[start9:end9]\n",
    "        text9 = text8.replace(term9,'')\n",
    "        indices9.append(index)\n",
    "    else:\n",
    "        text9 = text8\n",
    "    if text9.find(u'var NYTD = window.NYTD')>=0:\n",
    "        start10 = text9.find(u'var NYTD = window.NYTD')\n",
    "        end10 = start10+text9[start10:start10+660].rfind(u'});')+len(u'});')\n",
    "        term10 = text9[start10:end10]\n",
    "        text10 = text9.replace(term10,'')\n",
    "        indices10.append(index)\n",
    "    else:\n",
    "        text10 = text9\n",
    "    if text10.find(u'NYTD.NYTD.')>=0:\n",
    "        start11 = text10.find(u'NYTD.NYTD.')\n",
    "        end11 = start11+text10[start11:].find(u'1em;}')+len(u'1em;}')\n",
    "        term11 = text10[start11:end11]\n",
    "        text11 = text10.replace(term11,'')\n",
    "        indices11.append(index)\n",
    "    else:\n",
    "        text11 = text10\n",
    "    if text11.find(u'NYTD.NYTINT.')>=0:\n",
    "        start12 = text11.find(u'NYTD.NYTINT.')\n",
    "        end12 = start12+text11[start12:].find(u'});')+len(u'});')\n",
    "        term12 = text11[start12:end12]\n",
    "        text12 = text11.replace(term12,'')\n",
    "        indices12.append(index)\n",
    "    else:\n",
    "        text12 = text11\n",
    "    if text12.find(u'// < ![CDATA[')>=0:\n",
    "        start13 = text12.find(u'// < ![CDATA[')\n",
    "        end13 = start13+text12[start13:].find(u'// ]]>')+len(u'// ]]>')\n",
    "        term13 = text12[start13:end13]\n",
    "        text13 = text12.replace(term13,'')\n",
    "        indices13.append(index)\n",
    "    else:\n",
    "        text13 = text12\n",
    "    if text13.find(u'NYTD.jQuery(function()')>=0:\n",
    "        start14 = text13.find(u'NYTD.jQuery(function()')\n",
    "        end14 = start14+text13[start14:].find(u'});')+len(u'});')\n",
    "        term14 = text13[start14:end14]\n",
    "        text14 = text13.replace(term14,'')\n",
    "        indices14.append(index)\n",
    "    else:\n",
    "        text14 = text13\n",
    "    if text14.find(u'<!-- #screen_name_input')>=0:\n",
    "        start15 = text14.find(u'<!-- #screen_name_input')\n",
    "        end15 = start15+text14[start15:].find(u'// ]]>')+len(u'// ]]>')\n",
    "        term15 = text14[start15:end15]\n",
    "        text15 = text14.replace(term15,'')\n",
    "        indices15.append(index)\n",
    "    else:\n",
    "        text15 = text14\n",
    "    if 0<=text15.find(u'\\u2014')<=30:\n",
    "        if text15[0:3].isupper():\n",
    "            text16 = text15[text15.find(u'\\u2014')+1:]\n",
    "        else:\n",
    "            text16 = text15\n",
    "    else:\n",
    "        text16 = text15\n",
    "    text16 = u' '.join(text16.split())\n",
    "    newtexts.append(text16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
